{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43002542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "#importing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0513b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html(my_url):\n",
    "    try:\n",
    "        #reading the page html\n",
    "        uClient = uReq(my_url)\n",
    "        page_html = uClient.read()\n",
    "        uClient.close\n",
    "        #returns the page_html that's used for the html_reader function\n",
    "        return page_html\n",
    "    except Exception as e:\n",
    "        #if the html is not valid, goes to this step\n",
    "        print(e)\n",
    "        print(my_url)\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57381827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_reader(page_html):\n",
    "    #returns the CEO Pay, median employee pay, and CEO to employee pay ratio of a company from an html\n",
    "    try:\n",
    "        page_soup = soup(page_html, 'html.parser')\n",
    "        containers = page_soup.findAll(\"table\",{\"class\":\"table sa-table tablesaw-stack table-executive\"})\n",
    "        #finds all tables on the website (the CEO pay, median employee pay, and CEO to employee pay ratio are on a table)\n",
    "        container = containers[2]\n",
    "        #the container on the second index has the values we nee\n",
    "        values = container.find_all(\"tbody\")[0]\n",
    "        #turn the table into readable text\n",
    "        new_list = []\n",
    "        for x in str(values.get_text()).split(\"\\n\"):\n",
    "            if x != '':\n",
    "                new_list.append(x)\n",
    "        return new_list\n",
    "        #run a for loop to remove all empty lists\n",
    "        #This new list has CEO name on index 0, CEO pay on 2, Median employee pay on 4, and pay ratio on 6\n",
    "    except Exception as e:\n",
    "        #if the page_html is not valid, goes to this step\n",
    "        print(e)\n",
    "        print('html_reader error for' + my_url)    \n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd6119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing if the html_reader and read_html functions work\n",
    "html_reader(read_html('https://www1.salary.com/APPLE-INC-Executive-Salaries.html'))\n",
    "#We want the values on index 2, 4, and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All of these names are already edited \n",
    "df = pd.read_excel (r'C:s&p500_companies_names_final.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce2df2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fitting everything into the url on salary.com for constant scraping\n",
    "#Anything not caught by these url changes will be manually replaced in the spreadsheet\n",
    "'''\n",
    "df['Company'] = df['Company'].str.replace('.com','-com')\n",
    "df['Company'] = df['Company'].str.replace(' ','-')\n",
    "df['Company'] = df['Company'].str.replace('.','')\n",
    "df['Company'] = df['Company'].str.replace('-Class-A','')\n",
    "df['Company'] = df['Company'].str.replace('-Class-B','')\n",
    "df['Company'] = df['Company'].str.replace('-Class-C','')\n",
    "df['Company'] = df['Company'].str.replace('-Class-P','')\n",
    "df['Company'] = df['Company'].str.replace('oration','')\n",
    "df['Company'] = df['Company'].str.replace('INCORPORATED','INC')\n",
    "df['Company'] = df['Company'].str.replace('&','AND')\n",
    "df['Company'] = df['Company'].str.replace(r\"'\",'')\n",
    "df['Company'] = df['Company'].str.replace('COMPANY','CO')\n",
    "df['Company'] = df['Company'].str.replace('!','')\n",
    "df['Company'] = df['Company'].str.replace('INCORPORATED','INC')\n",
    "df['Company'] = df['Company'].str.lower()\n",
    "df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec2aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if the changes worked\n",
    "for company in df['Company'].values:\n",
    "    print(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating columns for the dataframe of -2 so we know if the value was not scraped\n",
    "df = df.assign(CEO_Pay=-2)\n",
    "df = df.assign(Median_Employee_Pay=-2)\n",
    "df = df.assign(CEO_Pay_Ratio=-2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dbe553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Changes values in all three columns \n",
    "bad_link_list=[]\n",
    "for index in range(len(df.index)):\n",
    "    #Loops through all rows of the dataframe\n",
    "    my_url='https://www1.salary.com/' + df.loc[index,'Company'] + '-Executive-Salaries.html'\n",
    "    #Each URL follows the pattern above for scraping\n",
    "    html_string=read_html(my_url)\n",
    "    ceo_worker_pay_list=html_reader(html_string)\n",
    "    if len(ceo_worker_pay_list)>6:\n",
    "        df.at[index, 'CEO_Pay'] = ceo_worker_pay_list[2]\n",
    "        #changes CEO_Pay column\n",
    "        df.at[index, 'Median_Employee_Pay'] = ceo_worker_pay_list[4]\n",
    "        #changes Median_Employee_Pay column\n",
    "        df.at[index, 'CEO_Pay_Ratio'] = ceo_worker_pay_list[6]\n",
    "        #changes CEO_Pay_Ratio column\n",
    "    else:\n",
    "        #makes a list of all links that did not work through the scrape\n",
    "        bad_link_list.append(my_url)\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ac3b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(bad_link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c946482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(bad_link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0960c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in bad_link_list:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c541b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf6994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Example1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36f125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
